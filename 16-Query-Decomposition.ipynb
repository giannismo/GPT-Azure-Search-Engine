{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubQuery(BaseModel):\n",
    "    \"\"\"Search over documents about bank institutions.\"\"\"\n",
    "\n",
    "    sub_query: str = Field(\n",
    "        ...,\n",
    "        description=\"A very specific query against documents.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system =\\\n",
    "\"\"\"\n",
    "You are an expert that generates multiple sub-questions.\n",
    "You have access to documents about bank institutions,\n",
    "with each document referencing a single bank institution.\n",
    "\n",
    "Perform query decomposition. Given a user question, break it down into distinct sub-questions per\n",
    "bank organization that you need to answer in order to respond to the original user question.\n",
    "Generate as many questions as the number of distinct bank entities you encounter\n",
    "in the original question.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-4\", temperature=0.5)\n",
    "llm_with_tools = llm.bind_functions([SubQuery])\n",
    "query_analyzer = prompt | llm_with_tools | StrOutputParser() | (lambda x: x.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_question = (\n",
    "    \"How many GRI requirements have been implemented by the Bank A, Bank B, and Bank C?\"\n",
    ")\n",
    "\n",
    "sub_questions = query_analyzer.invoke({\"question\": original_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub_questions)\n",
    "\n",
    "# Output:\n",
    "# [\n",
    "    # 'How many GRI requirements have been implemented by Bank A?', \n",
    "    # 'How many GRI requirements have been implemented by Bank B?', \n",
    "    # 'How many GRI requirements have been implemented by Bank C?',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sub_questions == [\"\"]:\n",
    "    print(\"No sub-questions were generated.\")\n",
    "    sub_questions = [original_question]\n",
    "else:\n",
    "    print(f\"Original question was decomposed into {len(sub_questions)} sub-questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.utils import create_docsearch_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent = create_docsearch_agent() # Dummy RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_results = []\n",
    "\n",
    "for i, sub_question in enumerate(sub_questions, start=1):\n",
    "    print(f\"Sub-question {i}: {sub_question}\")\n",
    "\n",
    "    for k in range(2):\n",
    "        print(f\"Attempt {k + 1}\")\n",
    "        try:\n",
    "            sub_response = rag_agent(sub_question)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            sub_response = \"No response found.\"\n",
    "            continue\n",
    "\n",
    "rag_results.append(sub_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "\n",
    "    return formatted_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = format_qa_pairs(sub_questions, rag_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\\\n",
    "\"\"\"\n",
    "Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use this information to answer the original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_response = final_rag_chain.invoke({\"context\": context, \"question\": original_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
